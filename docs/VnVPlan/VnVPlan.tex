\documentclass[12pt, titlepage]{article}

\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=blue,
    filecolor=black,
    linkcolor=red,
    urlcolor=blue
}
\usepackage[round]{natbib}

\input{../Comments}
\input{../Common}

\begin{document}

\title{Project Title: System Verification and Validation Plan for \progname{}} 
\author{Author Name}
\date{\today}
	
\maketitle

\pagenumbering{roman}

\section*{Revision History}
\begin{table}[hp]
	\caption{Revision History} \label{TblRevisionHistory}
	\begin{tabularx}{\textwidth}{llX}
		\toprule
		\textbf{Date} & \textbf{Developer(s)} & \textbf{Change}\\
		\midrule
		October 31 & Jarrod Colwell & Summary \& Objectives content added\\
		\bottomrule
	\end{tabularx}
\end{table}

\newpage

\tableofcontents

\listoftables
\wss{Remove this section if it isn't needed}

\listoffigures
\wss{Remove this section if it isn't needed}

\newpage

\section{Symbols, Abbreviations and Acronyms}

\renewcommand{\arraystretch}{1.2}
\begin{tabular}{l | l} 
  \toprule		
  \textbf{Symbol} & \textbf{Description}\\
  \midrule 
  T & Test\\
  V\&V & System Verification and Validation\\
  SRS & Software Requirements Specification\\
  UI & User Interface\\
  
  \bottomrule
\end{tabular}\\

\wss{symbols, abbreviations or acronyms -- you can simply reference the SRS
  \citep{SRS} tables, if appropriate}

\newpage

\pagenumbering{arabic}

This document ... \wss{provide an introductory blurb and roadmap of the
  Verification and Validation plan}

\section{General Information}

\subsection{Summary}
This document describes the plan to verify and validate that Flick Picker meets the defined requirements and specifications. Additionally, this document will validate that Flick Picker fulfills its intended purpose of recommending compatible Movies, TV Shows, or Anime to an individual or a group.

\subsubsection{Front End Testing}
\begin{itemize}
	\item Account Creation - Web page that facilitates user account creation
	\item User Preferences - Web page that facilitates user preference settings
	\item Group Creation - Web page that facilitates the creation of groups
	\item Recommendation - Web page that displays recommendations for an individual or group
\end{itemize}
\subsubsection{Back End Testing}
\begin{itemize}
	\item Database Access - Accessing the database to find user preferences or information pertaining to Movies, TV Shows, or Anime
	\item Recommendation Algorithm - The algorithm responsible for finding the best Movies, TV Shows, or Anime for an individual or group
	\item API Data - The accessing, storage, and usage of external data from various APIs
\end{itemize}

% \wss{Say what software is being tested.  Give its name and a brief overview of its general functions.}

\subsection{Objectives}
\subsubsection{Requirements}
The first objective involves verifying that Flick Picker meets requirements outlined in our SRS document and validating that the behaviour present is desirable. This includes the functional requirements (e.g. Authentication Requirements) and the non-functional requirements (e.g. Appearance Requirements). This objective will build confidence in the correctness of Flick Picker along with validating that security and usability standards are met.

\subsubsection{UI Elements}
The second objective of the V\&V document involves the validation of navigability and functionality of UI elements. Each menu described in the 'Front End Testing' section above must be functional for users. Additionally, users must be able to navigate to each page individually. This objective ensures that usability and response time standards are met. 

\subsubsection{External Connections}
The final objective of the V\&V document is to ensure that all external connections of Flick Picker (e.g. APIs, Database) work as intended. This objective ensures that the interoperability of Flick Picker is adequate. 

%\wss{State what is intended to be accomplished.  The objective will be around the qualities that are most important for your project.  You might have something like: ``build confidence in the software correctness,'' ``demonstrate adequate usability.'' etc.  You won't list all of the qualities, just those that are most important.}

\subsection{Relevant Documentation}

\wss{Reference relevant documentation.  This will definitely include your SRS
  and your other project documents (MG, MIS, etc).  You can include these even
  before they are written, since by the time the project is done, they will be
  written.}

\citet{SRS}

\section{Plan}

\wss{Introduce this section.   You can provide a roadmap of the sections to
  come.}

\subsection{Verification and Validation Team}

\wss{You, your classmates and the course instructor.  Maybe your supervisor.
  You shoud do more than list names.  You should say what each person's role is
  for the project.  A table is a good way to summarize this information.}

\subsection{SRS Verification Plan}

\wss{List any approaches you intend to use for SRS verification.  This may just
  be ad hoc feedback from reviewers, like your classmates, or you may have
  something more rigorous/systematic in mind..}

\wss{Remember you have an SRS checklist}

\subsection{Design Verification Plan}

\wss{Plans for design verification}

\wss{The review will include reviews by your classmates}

\wss{Remember you have MG and MIS checklists}

\subsection{Implementation Verification Plan}

\wss{You should at least point to the tests listed in this document and the unit
  testing plan.}

\wss{In this section you would also give any details of any plans for static verification of
  the implementation.  Potential techniques include code walkthroughs, code
  inspection, static analyzers, etc.}

\subsection{Automated Testing and Verification Tools}

\wss{What tools are you using for automated testing.  Likely a unit testing
  framework and maybe a profiling tool, like ValGrind.  Other possible tools
  include a static analyzer, make, continuous integration tools, test coverage
  tools, etc.  Explain your plans for summarizing code coverage metrics.
  Linters are another important class of tools.  For the programming language
  you select, you should look at the available linters.  There may also be tools
  that verify that coding standards have been respected, like flake9 for
  Python.}

\wss{The details of this section will likely evolve as you get closer to the
  implementation.}

\subsection{Software Validation Plan}

\wss{If there is any external data that can be used for validation, you should
  point to it here.  If there are no plans for validation, you should state that
  here.}

\section{System Test Description}
	
\subsection{Tests for Functional Requirements}

\wss{Subsets of the tests may be in related, so this section is divided into
  different areas.  If there are no identifiable subsets for the tests, this
  level of document structure can be removed.}

\wss{Include a blurb here to explain why the subsections below
  cover the requirements.  References to the SRS would be good.}

\subsubsection{Area of Testing1}

\wss{It would be nice to have a blurb here to explain why the subsections below
  cover the requirements.  References to the SRS would be good.  If a section
  covers tests for input constraints, you should reference the data constraints
  table in the SRS.}
		
\paragraph{Title for Test}

\begin{enumerate}

\item{test-id1\\}

Control: Manual versus Automatic
					
Initial State: 
					
Input: 
					
Output: \wss{The expected result for the given inputs}

Test Case Derivation: \wss{Justify the expected value given in the Output field}
					
How test will be performed: 
					
\item{test-id2\\}

Control: Manual versus Automatic
					
Initial State: 
					
Input: 
					
Output: \wss{The expected result for the given inputs}

Test Case Derivation: \wss{Justify the expected value given in the Output field}

How test will be performed: 

\end{enumerate}

\subsubsection{Area of Testing2}

...

\subsection{Tests for Nonfunctional Requirements}

\wss{The nonfunctional requirements for accuracy will likely just reference the
  appropriate functional tests from above.  The test cases should mention
  reporting the relative error for these tests.}

\wss{Tests related to usability could include conducting a usability test and
  survey.}

The tests below are based directly on the NFR sections from the SRS. Not all tests will be automated due to the nature of the requirement being tested.

\subsubsection{Area of Testing: Look and Feel}

Contains all the tests for the Look and Feel NFR.

\paragraph*{Look and Feel T 1: Appearance}
\begin{itemize}
	\item[Control:] Functional, Dynamic, Manual, or Static
	\item[Initial State:] Initial State
	\item[Input:] Input
	\item[Output:] Output
	\item[Derivation:] Test Case Derivation
	\item[Execution:] Automated Selenium Test | API Test
\end{itemize}

\paragraph*{Look and Feel T 2: Style}
\begin{itemize}
	\item[Control:] Functional, Dynamic, Manual, or Static
	\item[Initial State:] Initial State
	\item[Input:] Input
	\item[Output:] Output
	\item[Derivation:] Test Case Derivation
	\item[Execution:] Automated Selenium Test | API Test
\end{itemize}

\subsubsection{Area of Testing: Usability and Humanity}

Contains all the tests for the Usability and Humanity NFR.

\paragraph*{Usability and Humanity T 1: Ease of Use}
\begin{itemize}
	\item[Control:] Functional, Dynamic, Manual, or Static
	\item[Initial State:] Initial State
	\item[Input:] Input
	\item[Output:] Output
	\item[Derivation:] Test Case Derivation
	\item[Execution:] Automated Selenium Test | API Test
\end{itemize}

\paragraph*{Usability and Humanity T 2: Learning}
\begin{itemize}
	\item[Control:] Functional, Dynamic, Manual, or Static
	\item[Initial State:] Initial State
	\item[Input:] Input
	\item[Output:] Output
	\item[Derivation:] Test Case Derivation
	\item[Execution:] Automated Selenium Test | API Test
\end{itemize}

\subsubsection{Area of Testing: Performance}

Contains all the tests for the Performance NFR.

\paragraph*{Performance T 1: Speed and Latency}
\begin{itemize}
	\item[Control:] Functional, Dynamic, Manual, or Static
	\item[Initial State:] Initial State
	\item[Input:] Input
	\item[Output:] Output
	\item[Derivation:] Test Case Derivation
	\item[Execution:] Automated Selenium Test | API Test
\end{itemize}

\paragraph*{Performance T 2: Safety-Critical}
\begin{itemize}
	\item[Control:] Functional, Dynamic, Manual, or Static
	\item[Initial State:] Initial State
	\item[Input:] Input
	\item[Output:] Output
	\item[Derivation:] Test Case Derivation
	\item[Execution:] Automated Selenium Test | API Test
\end{itemize}

\paragraph*{Performance T 3: Precision or Accuracy}
\begin{itemize}
	\item[Control:] Functional, Dynamic, Manual, or Static
	\item[Initial State:] Initial State
	\item[Input:] Input
	\item[Output:] Output
	\item[Derivation:] Test Case Derivation
	\item[Execution:] Automated Selenium Test | API Test
\end{itemize}

\paragraph*{Performance T 4: Reliability and Availability}
\begin{itemize}
	\item[Control:] Functional, Dynamic, Manual, or Static
	\item[Initial State:] Initial State
	\item[Input:] Input
	\item[Output:] Output
	\item[Derivation:] Test Case Derivation
	\item[Execution:] Automated Selenium Test | API Test
\end{itemize}


\paragraph*{Performance T 5: Robustness or Fault-Tolerance}
\begin{itemize}
	\item[Control:] Functional, Dynamic, Manual, or Static
	\item[Initial State:] Initial State
	\item[Input:] Input
	\item[Output:] Output
	\item[Derivation:] Test Case Derivation
	\item[Execution:] Automated Selenium Test | API Test
\end{itemize}

\paragraph*{Performance T 6: Capacity}
\begin{itemize}
	\item[Control:] Functional, Dynamic, Manual, or Static
	\item[Initial State:] Initial State
	\item[Input:] Input
	\item[Output:] Output
	\item[Derivation:] Test Case Derivation
	\item[Execution:] Automated Selenium Test | API Test
\end{itemize}


\paragraph*{Performance T 7: Scalability or Extensibility???}
\begin{itemize}
	\item[Control:] Functional, Dynamic, Manual, or Static
	\item[Initial State:] Initial State
	\item[Input:] Input
	\item[Output:] Output
	\item[Derivation:] Test Case Derivation
	\item[Execution:] Automated Selenium Test | API Test
\end{itemize}

\subsubsection{Area of Testing: Operational and Environmental}

Contains all the tests for the Operational and Environmental NFR.

\paragraph*{Operational and Environmental T 1: Interfacing with Adjacent Systems}
\begin{itemize}
	\item[Control:] Functional, Dynamic, Manual, or Static
	\item[Initial State:] Initial State
	\item[Input:] Input
	\item[Output:] Output
	\item[Derivation:] Test Case Derivation
	\item[Execution:] Automated Selenium Test | API Test
\end{itemize}

\subsubsection{Area of Testing: Maintainability and Support}

Contains all the tests for the Operational and Environmental NFR.

\paragraph*{Maintainability and Support T 1: Adaptability}
\begin{itemize}
	\item[Control:] Functional, Dynamic, Manual, or Static
	\item[Initial State:] Initial State
	\item[Input:] Input
	\item[Output:] Output
	\item[Derivation:] Test Case Derivation
	\item[Execution:] Automated Selenium Test | API Test
\end{itemize}

\subsubsection{Area of Testing: Security}

Contains all the tests for the Operational and Environmental NFR.

\paragraph*{Security T 1: Access}
\begin{itemize}
	\item[Control:] Functional, Dynamic, Manual, or Static
	\item[Initial State:] Initial State
	\item[Input:] Input
	\item[Output:] Output
	\item[Derivation:] Test Case Derivation
	\item[Execution:] Automated Selenium Test | API Test
\end{itemize}

\paragraph*{Security T 2: Integrity}
\begin{itemize}
	\item[Control:] Functional, Dynamic, Manual, or Static
	\item[Initial State:] Initial State
	\item[Input:] Input
	\item[Output:] Output
	\item[Derivation:] Test Case Derivation
	\item[Execution:] Automated Selenium Test | API Test
\end{itemize}

\paragraph*{Security T 3: Privacy}
\begin{itemize}
	\item[Control:] Functional, Dynamic, Manual, or Static
	\item[Initial State:] Initial State
	\item[Input:] Input
	\item[Output:] Output
	\item[Derivation:] Test Case Derivation
	\item[Execution:] Automated Selenium Test | API Test
\end{itemize}

\subsection{Traceability Between Test Cases and Requirements}

\wss{Provide a table that shows which test cases are supporting which
  requirements.}

\section{Unit Test Description}

\wss{Reference your MIS and explain your overall philosophy for test case
  selection.}  
\wss{This section should not be filled in until after the MIS has
  been completed.}

\subsection{Unit Testing Scope}

\wss{What modules are outside of the scope.  If there are modules that are
  developed by someone else, then you would say here if you aren't planning on
  verifying them.  There may also be modules that are part of your software, but
  have a lower priority for verification than others.  If this is the case,
  explain your rationale for the ranking of module importance.}

\subsection{Tests for Functional Requirements}

\wss{Most of the verification will be through automated unit testing.  If
  appropriate specific modules can be verified by a non-testing based
  technique.  That can also be documented in this section.}

\subsubsection{Module 1}

\wss{Include a blurb here to explain why the subsections below cover the module.
  References to the MIS would be good.  You will want tests from a black box
  perspective and from a white box perspective.  Explain to the reader how the
  tests were selected.}

\begin{enumerate}

\item{test-id1\\}

Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
  be automatic}
					
Initial State: 
					
Input: 
					
Output: \wss{The expected result for the given inputs}

Test Case Derivation: \wss{Justify the expected value given in the Output field}

How test will be performed: 
					
\item{test-id2\\}

Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
  be automatic}
					
Initial State: 
					
Input: 
					
Output: \wss{The expected result for the given inputs}

Test Case Derivation: \wss{Justify the expected value given in the Output field}

How test will be performed: 

\item{...\\}
    
\end{enumerate}

\subsubsection{Module 2}

...

\subsection{Tests for Nonfunctional Requirements}

\wss{If there is a module that needs to be independently assessed for
  performance, those test cases can go here.  In some projects, planning for
  nonfunctional tests of units will not be that relevant.}

\wss{These tests may involve collecting performance data from previously
  mentioned functional tests.}

\subsubsection{Module ?}
		
\begin{enumerate}

\item{test-id1\\}

Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
  be automatic}
					
Initial State: 
					
Input/Condition: 
					
Output/Result: 
					
How test will be performed: 
					
\item{test-id2\\}

Type: Functional, Dynamic, Manual, Static etc.
					
Initial State: 
					
Input: 
					
Output: 
					
How test will be performed: 

\end{enumerate}

\subsubsection{Module ?}

...

\subsection{Traceability Between Test Cases and Modules}

\wss{Provide evidence that all of the modules have been considered.}
				
\bibliographystyle{plainnat}

\bibliography{../../refs/References}

\newpage

\section{Appendix}

This is where you can place additional information.

\subsection{Symbolic Parameters}

The definition of the test cases will call for SYMBOLIC\_CONSTANTS.
Their values are defined in this section for easy maintenance.

\subsection{Usability Survey Questions?}

\wss{This is a section that would be appropriate for some projects.}

\end{document}